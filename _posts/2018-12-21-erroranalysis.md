# 机器学习泛化误差

## 1. 概念解释

### 1.1 期望损失风险
> 模型在任意未知测试样本的表现

$$min_{g\in \mathcal{G}} L(g)=\mathbb{E_{x,y\sim P_{x,y}}}l(g;x,y)$$

其中 $\mathcal{G}$ 是预先给定的一个函数族, $g$ 是函数组中的一个函数, $P_{x,y}$ 是数据的真是分布,$l$ 是损失函数. 

### 1.2 经验损失风险
> 模型在训练集数据上的表现


假设有训练数据集 $S_n=\{(x_i,y_i),...,(x_n,y_n)\}$

$$min_{g\in\mathcal{G}} L(\hat{g}_n)=\frac{1}{n}\sum_{i=1}^n l(g;x_i,y_i)$$


### 1.3 结构损失风险
> 函数空间$\mathcal{G}$受限时,即有正则项时的表现,此时函数子空间范数小于$c$.



$$
min_{g\in\mathcal{G}_c} L(\hat{g}_n)=\frac{1}{n}\sum_{i=1}^n l(g;x_i,y_i) \\
\mathcal{G}_c=\{g:g\in \mathcal{G},\|g\| \le c \}
$$

### 1.4 泛化误差
> 优化算法对结构损失风险最小化问题求解,在算法结束的第 $T$ 次迭代后输出模型 $\hat{g_{T}}$, 用这个模型对位置数据预测的误差即为泛化误差.




**总结:**
我们最希望最小化期望损失风险 $L(g)$ ,但是数据的真实分布可能永远也不知道,所以只能根据观测到的数据,最小化经验损失风险 $L(\hat{g}_n)$.最后为了防止过拟合,我们限制了模型复杂度,取用最小化结构损失风险.

## 2. 泛化误差的分解

假设存在一个最优模型 $g^{*}$, 那我们肯定希望模型的泛化误差 $L(\hat{g_{T}})$ 和最优模型对应的期望风险 $L(g^*)$ 之差 $L(\hat{g_{T}})-L(g^*)$ 越小越好, 我们对其分解:
$$L(\hat{g_T})-L(g^*)=L(\hat{g_T})-L(\hat{g}_n)+L(\hat{g}_n)-L(g_{\mathcal{G}}^*)+L(g_{\mathcal{G}}^*)-L(g^*)$$
其中:
1. $g_{\mathcal{G}}^*$ 表示函数族 $\mathcal{G}$ 中使得期望风险最小的模型.
2. $\hat{g}_n$表示函数族 $\mathcal{G}$ 中使得结构风险最小的模型.

现分解如下:

- $L(\hat{g_T})-L(\hat{g}_n)$ 称为优化误差,衡量优化算法迭代$T$轮后输出的模型与最小化结构风险模型的差别. 由优化算法的局限性带来的, 与选用的优化算法,数据量大小,迭代轮数以及函数空间有关.
- $L(\hat{g}_n)-L(g_{\mathcal{G}}^*)$ 称为估计误差. 衡量最小化结构风险模型和最小化期望风险模型的差别, 由训练集的局限性带来的. 与数据集大小和函数复杂度都有关系.
- $L(g_{\mathcal{G}}^*)-L(g^*)$ 称为近似误差. 衡量最小化经验风险模型和全局最优期望风险模型的差别, 与函数空间的表达力有关.


## 3. 泛化误差分析
定性的分析, 函数空间增大也就是模型越复杂时,近似误差减小,估计误差增大; 数据量增大时, 估计误差减小, 优化误差变大; 迭代轮数增大时, 优化误差减小. 
可以看出减少泛化误差主要围绕,模型复杂度,数据量大小,优化算法这几个方向. 那么如何调节这些因素使得最终的泛化误差最小呢? 这需要进行定量分析.

##　**参考资料:**

- 分布式机器学习 算法理论与实践，刘铁岩.





